# News Aggregator

A scalable, microservices-based news aggregation system built with Go, featuring real-time data collection, processing, and search capabilities.

## ğŸš€ Features

- **Microservices Architecture**: Loosely coupled services for scalability
- **Multi-Source Data Collection**: RSS feeds, APIs, and web scraping
- **Real-time Processing**: Async processing with message queues
- **Advanced Search**: Full-text search with Elasticsearch
- **Content Intelligence**: Automatic categorization, sentiment analysis, and deduplication
- **RESTful API**: Clean API with authentication and rate limiting
- **Monitoring**: Comprehensive metrics and health checks
- **Containerized**: Docker-based deployment with orchestration

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Gateway   â”‚    â”‚ Data Collector  â”‚    â”‚   Processor     â”‚
â”‚   (Port 8080)   â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚           â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚PostgreSQLâ”‚    â”‚   Redis     â”‚    â”‚ RabbitMQ    â”‚    â”‚Elasticsearch â”‚
    â”‚(Port 5432)â”‚    â”‚(Port 6379) â”‚    â”‚(Port 5672) â”‚    â”‚ (Port 9200)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

1. **API Gateway**: HTTP routing, authentication, rate limiting, and circuit breaking
2. **Data Collection Layer**: Scheduled fetchers with pluggable source adapters
3. **Processing Pipeline**: Message queue-based async processing with transformers
4. **Storage Layer**: PostgreSQL, Redis cache, and Elasticsearch search

## ğŸ› ï¸ Technology Stack

- **Language**: Go 1.21+
- **Web Framework**: Gin
- **Database**: PostgreSQL with pgx driver
- **Cache**: Redis
- **Message Queue**: RabbitMQ
- **Search**: Elasticsearch
- **Monitoring**: Prometheus + Grafana
- **Containerization**: Docker + Docker Compose
- **Reverse Proxy**: Nginx

## ğŸ“‹ Prerequisites

- Go 1.21 or higher
- Docker and Docker Compose
- Make (optional, for convenience commands)

## ğŸš€ Quick Start

### Using Docker (Recommended)

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd NEWSAggregator
   ```

2. **Start all services**
   ```bash
   make docker-run
   # or
   docker-compose up -d
   ```

3. **Wait for services to initialize** (about 30-60 seconds)

4. **Verify services are running**
   ```bash
   make health-check
   # or
   curl http://localhost:8080/health
   ```

### Manual Setup

1. **Install dependencies**
   ```bash
   make deps
   ```

2. **Start infrastructure services**
   ```bash
   docker-compose up -d postgres redis rabbitmq elasticsearch
   ```

3. **Run services locally**
   ```bash
   # Terminal 1 - API Gateway
   make run-api
   
   # Terminal 2 - Data Collector
   make run-collector
   
   # Terminal 3 - Processor
   make run-processor
   ```

## ğŸ“Š Monitoring and Observability

### Access Monitoring Dashboards

- **API Health**: http://localhost:8080/health
- **Prometheus Metrics**: http://localhost:9090
- **Grafana Dashboard**: http://localhost:3000 (admin/admin)
- **RabbitMQ Management**: http://localhost:15672 (guest/guest)

### Key Metrics

- HTTP request rates and latencies
- News processing throughput
- Source fetch success rates
- Queue depths and processing times
- Database connection pools
- Error rates by component

## ğŸ”§ Configuration

Configuration is managed through YAML files and environment variables:

- **Config File**: `configs/config.yaml`
- **Environment Variables**: Override any config value using `SECTION_KEY` format

### Key Configuration Sections

```yaml
# Server settings
server:
  address: ":8080"
  read_timeout: 30
  write_timeout: 30

# Database connection
database:
  host: localhost
  port: 5432
  user: postgres
  password: postgres
  database: news_aggregator

# News sources
sources:
  - name: "BBC News RSS"
    type: "rss"
    url: "http://feeds.bbci.co.uk/news/rss.xml"
    schedule: "15m"
    enabled: true
```

## ğŸ“¡ API Documentation

### Authentication

```bash
# Register a new user
curl -X POST http://localhost:8080/api/v1/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "username": "testuser",
    "password": "password123",
    "first_name": "Test",
    "last_name": "User"
  }'

# Login
curl -X POST http://localhost:8080/api/v1/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "password": "password123"
  }'
```

### News Endpoints

```bash
# Get latest news
curl http://localhost:8080/api/v1/news

# Get news by category
curl http://localhost:8080/api/v1/news?category=technology

# Search news
curl http://localhost:8080/api/v1/search?q=artificial+intelligence

# Get categories
curl http://localhost:8080/api/v1/categories
```

### Protected Endpoints (require JWT token)

```bash
# Get user profile
curl -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  http://localhost:8080/api/v1/profile

# Add bookmark
curl -X POST -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"news_id": "article-id"}' \
  http://localhost:8080/api/v1/bookmarks
```

### Admin Endpoints (require JWT token)

```bash
# Manual database cleanup (removes articles older than 2 days)
curl -X POST http://localhost:8082/api/v1/admin/cleanup \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"

# Check log status and rotation info
curl -X POST http://localhost:8082/api/v1/admin/cleanup/logs \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run tests with coverage
make test-coverage

# Run linter
make lint

# API endpoint testing
make api-test
```

## ğŸ”„ Data Sources

### Supported Source Types

1. **RSS Feeds**: Automatic parsing of RSS/Atom feeds
2. **REST APIs**: JSON API integration with configurable endpoints
3. **Web Scraping**: HTML content extraction (extensible)

### Adding New Sources

Add sources to `configs/config.yaml`:

```yaml
sources:
  - name: "Custom News Source"
    type: "rss"  # or "api" or "scraper"
    url: "https://example.com/feed.xml"
    schedule: "30m"
    rate_limit: 10
    headers:
      User-Agent: "NewsAggregator/1.0"
    enabled: true
```

## ğŸ”§ Development

### Project Structure

```
â”œâ”€â”€ cmd/                    # Application entrypoints
â”‚   â”œâ”€â”€ api-gateway/
â”‚   â”œâ”€â”€ data-collector/
â”‚   â””â”€â”€ processor/
â”œâ”€â”€ internal/               # Private application code
â”‚   â”œâ”€â”€ config/            # Configuration management
â”‚   â”œâ”€â”€ gateway/           # API Gateway implementation
â”‚   â”œâ”€â”€ collector/         # Data collection logic
â”‚   â”œâ”€â”€ processor/         # Processing pipeline
â”‚   â”œâ”€â”€ models/            # Data models
â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”œâ”€â”€ repository/        # Data access layer
â”‚   â”œâ”€â”€ middleware/        # HTTP middleware
â”‚   â””â”€â”€ health/            # Health checks
â”œâ”€â”€ pkg/                   # Public packages
â”‚   â”œâ”€â”€ logger/           # Logging utilities
â”‚   â”œâ”€â”€ metrics/          # Metrics collection
â”‚   â””â”€â”€ queue/            # Message queue abstraction
â”œâ”€â”€ configs/              # Configuration files
â”œâ”€â”€ scripts/             # Utility scripts
â””â”€â”€ docker-compose.yml   # Container orchestration
```

### Adding New Features

1. **New Data Source Type**: Implement the `DataSource` interface in `internal/datasources/`
2. **New Transformer**: Implement the `Transformer` interface in `internal/processor/`
3. **New API Endpoint**: Add routes in `internal/gateway/gateway.go`

### Code Style

- Follow Go conventions and use `gofmt`
- Use structured logging with zerolog
- Include comprehensive error handling
- Write tests for new functionality

## ğŸš¢ Deployment

### Production Deployment

1. **Environment Configuration**
   ```bash
   export ENVIRONMENT=production
   export JWT_SECRET_KEY=your-secure-secret-key
   export DATABASE_PASSWORD=secure-password
   ```

2. **Build and Deploy**
   ```bash
   make docker-build
   make docker-run
   ```

3. **Configure Reverse Proxy** (Nginx configuration provided)

4. **Set up SSL/TLS** certificates

5. **Configure Monitoring** alerts and dashboards

### Scaling Considerations

- **Horizontal Scaling**: Run multiple instances behind a load balancer
- **Database Scaling**: Use read replicas for queries
- **Cache Scaling**: Redis clustering for high availability
- **Queue Scaling**: RabbitMQ clustering for message durability

## ğŸ”’ Security

- JWT-based authentication
- Rate limiting per IP and user
- Input validation and sanitization
- SQL injection prevention with parameterized queries
- CORS configuration
- Security headers via Nginx

## ğŸ“ˆ Performance

### Optimization Features

- Connection pooling for databases
- HTTP client connection reuse
- Batch processing for database operations
- Compression for API responses
- Caching strategies with Redis
- Elasticsearch indexing optimization

### Benchmarks

- API Gateway: ~1000 req/s per instance
- Data Collection: ~100 sources per minute
- Processing Pipeline: ~500 articles per second
- Search Performance: <100ms for typical queries

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

- **Issues**: GitHub Issues for bug reports and feature requests
- **Documentation**: Check the `/docs` directory for detailed guides
- **Community**: Join our discussions for questions and support

## ğŸ—ºï¸ Roadmap

- [ ] WebSocket real-time notifications
- [ ] Machine learning-based content recommendations
- [ ] Multi-language support
- [ ] Advanced analytics dashboard
- [ ] Mobile app API endpoints
- [ ] Content summarization with AI
- [ ] Social media integration
- [ ] Email digest functionality

---

**Built with â¤ï¸ using Go and modern cloud-native technologies.**
